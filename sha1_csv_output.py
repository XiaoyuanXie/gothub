#!/usr/bin/env python
# Brandon Heller
# Parse processed files/directories generated by gothub.sh and output CSV
# suitable for Tableau exploration.

# Eventual desired output format:
#
# commits: [ {sha, author, email, lat, long, location, time zone, local time,
# [repos], time, description, [parent shas], [optional derived parent fields for
# convenience]}, {}... ]

import json
from optparse import OptionParser
import os
import time
from os.path import isfile

# Default location of raw and log dirs for gothub
DEF_DATA_DIR_NAME = "gothub"
DEF_DATA_DIR_PATH = "/home/cs448b"
DEF_DATA_DIR = os.path.join(DEF_DATA_DIR_PATH, DEF_DATA_DIR_NAME)

# Path for output files
DEF_OUTPUT_PATH = './'

# Output filename extension
OUTPUT_EXT = '.csv'

# Hard # of max commits
HARD_MAX_COMMITS = 1000000

# Default max number of commits to parse before outputting csv
# Set to None to read all.
DEF_MAX_COMMITS = None

# Commits to read until printing out current commit
PRINT_INTERVAL = 100


class CommitJSONBuilder:

    def __init__(self):
        self.fields = ['sha1', 'author', 'location', 'lat', 'long',
                       'committed_date', 'authored_date', 'parents_total',
                       'parents_str', 'branches_total', 'branches_str']
        # double-quoted fields
        self.fields_to_quote = ['location', 'committed_date', 'authored_date',
                                'parents_str', 'branches_str']
        self.parse_args()
        self.build_dirs()
        self.build_json()

    def parse_args(self):
        opts = OptionParser()
        opts.add_option("--data_dir", "-d", type = "string",
                        default = DEF_DATA_DIR,
                        help = "location of raw and log dirs")
        opts.add_option('--file_name','-f', type = 'string', default = None,
                        help = "output file name; default is gothub... .csv")
        opts.add_option("--max_commits", "-m", type = "int",
                        default = DEF_MAX_COMMITS,
                        help = "max commits to parse; default is all")
        opts.add_option("--no-output", action = "store_false",
                        default = True, dest = "output",
                        help = "print output at end?")
        options, arguments = opts.parse_args()

        if options.data_dir[-1] == '/':
            # Strip trailing slash, which causes basename to fail.
            options.data_dir = options.data_dir[:-1]

        # Verify log dir
        if not os.path.isdir(options.data_dir):
            raise Exception("invalid data dir:", options.data_dir)

        if not options.file_name:
            options.file_name = os.path.basename(options.data_dir)
            if options.max_commits:
                options.file_name += '-%i' % options.max_commits
            options.file_name += OUTPUT_EXT

        self.options = options

    def build_dirs(self):
        data_dir = self.options.data_dir
        self.done_file = os.path.join(data_dir, 'log/commits/done')
        log_dir = os.path.join(data_dir, 'log')
        self.dirs = {
            'sha1': {
                'author': os.path.join(log_dir, 'commits/author'),
                'committed_date': os.path.join(log_dir, 'commits/committed_date'),
                'authored_date': os.path.join(log_dir, 'commits/authored_date'),
                'parents': os.path.join(log_dir, 'commits/parents'),
                'branches': os.path.join(log_dir, 'commits/branches')
            },
            'user': {
                'location': os.path.join(log_dir, 'user/location'),
                'geocode': os.path.join(log_dir, 'user/geocode')
            }
        }

    def get_field(self, commit, input_type, field, key):
        path = os.path.join(self.dirs[input_type][field], key)
        if isfile(path):
            value = open(path, 'r').read().rstrip()
            commit[field] = value
            return value
        else:
            #print "ERROR: missing %s for %s/%s" % (key, input_type, field)
            return None

    def get_field_lines(self, commit, input_type, field, key):
        path = os.path.join(self.dirs[input_type][field], key)
        if isfile(path):
            value = open(path, 'r').readlines()
            for i, line in enumerate(value):
                value[i] = line.rstrip()
            commit[field] = value
            return value
        else:
            #print "ERROR: missing %s for %s/%s" % (key, input_type, field)
            return None

    def build_json(self):
        # Create json output

        commits = []  # list of commits with object data
        sha1s = open(self.done_file, 'r').readlines()
        assert sha1s

        print "[time] [commits parsed]"
        i = 0
        error = False
        start = time.time()
        for sha1 in sha1s:

            c = {}  # key/val pairs of interest            
            for field in self.fields:
                c[field] = 'unknown'
            sha1 = sha1.rstrip()
            c['sha1'] = sha1

            if self.get_field(c, 'sha1', 'author', sha1) != None:
                if self.get_field(c, 'user', 'location', c['author']) != None:
                    if self.get_field(c, 'user', 'geocode', c['author']) != None:
                        c['lat'], c['long'] = c['geocode'].split(',')
            self.get_field(c, 'sha1', 'committed_date', sha1)
            self.get_field(c, 'sha1', 'authored_date', sha1)
            if self.get_field_lines(c, 'sha1', 'parents', sha1) != None:
                c['parents_total'] = str(len(c['parents']))
                c['parents_str'] = ', '.join(c['parents'])
            if self.get_field_lines(c, 'sha1', 'branches', sha1) != None:
                c['branches_total'] = str(len(c['branches']))
                c['branches_str'] = ', '.join(c['branches'])
            # TODO: extract time zone from date.
            # example date: 2008-01-20T14:49:00-08:00
            # Not sure if the time is GMT and the shift is at the end?
            commits.append(c)

            i += 1
            if i == HARD_MAX_COMMITS:
                break
            if self.options.max_commits and (i == self.options.max_commits):
                break

            if i % PRINT_INTERVAL == 0:
                elapsed = float(time.time() - start)
                print '%0.3f %i' % (elapsed, i)

        if not error and self.options.output:
            output = open(DEF_OUTPUT_PATH + self.options.file_name, 'w')
            commits_csv = self.build_csv(commits)
            #print commits_csv
            output.write(commits_csv)
            #print json.dumps(commits)
        assert i == len(commits)
        print "read %i commits" % i
        elapsed = float(time.time() - start)
        print "%0.2f commits per second" % (i / elapsed)

    def build_csv(self, commits):
        crlf = '\r\n' # yes, add Windows line ending
        s = ""
        for i, field in enumerate(self.fields):
            s += field
            if i < len(self.fields) - 1:
                s += ', '
        s += crlf
        for commit in commits:
            for i, field in enumerate(self.fields):
                if field in self.fields_to_quote:
                    s += '"' + commit[field] + '"'
                    if i < len(self.fields) - 1:
                        s += ', '
                else:
                    s += commit[field]
                    if i < len(self.fields) - 1:
                        s += ', '
            s += crlf
        return s

if __name__ == "__main__":
    CommitJSONBuilder()
